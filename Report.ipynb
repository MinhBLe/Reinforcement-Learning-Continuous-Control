{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "---\n",
    "This project is based on the DDPG (Deep Deterministic Policy Gradient) architecture [DDPG-Bipedal Udacity project repo](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal). An immplementation improvement version using [Combined Experience Replay (CER)](https://arxiv.org/abs/1712.01275) is added for learning efficiency. \n",
    "\n",
    "## State and Action Spaces\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector must be a number between -1 and 1.\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "At each episodical training step, the agent first saves observations using a large-size Replay Buffer of (long) past experiences and optionally another small-size Replay Buffer of recent experiences, respectively. Then, the agent learns on random samples from these buffers. To explore environment, a stochastic Ornsteinâ€“Uhlenbeck process is added output action. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Training is completed when the average reward over the last 100 episodes, and over all agents, is at least +30.0.\n",
    "\n",
    "### DDPG Hyper Parameters\n",
    "- n_episodes (int): maximum number of training episodes (default 200).\n",
    "- num_agents: number of agents in the environment (20).\n",
    "\n",
    "\n",
    "### DDPG Agent Hyper Parameters\n",
    "\n",
    "- BUFFER_SIZE (int): large replay buffer size\n",
    "- CER_SIZE (int): small replay buffer for recent experiences\n",
    "- BATCH_SIZE (int): mini batch size\n",
    "- GAMMA (float): discount factor\n",
    "- TAU (float): for soft update of target parameters\n",
    "- LR_ACTOR (float): learning rate for optimizer\n",
    "- LR_CRITIC (float): learning rate for optimizer\n",
    "- WEIGHT_DECAY (float): L2 weight decay\n",
    "- N_LEARN_UPDATES (int): number of learning updates\n",
    "- N_TIME_STEPS (int): every n time step do update\n",
    "\n",
    "\n",
    "Where \n",
    "`BUFFER_SIZE = int(1e6)`, `CER_SIZE = int(2e4)`, `BATCH_SIZE = 128`, `GAMMA = 0.99`, `TAU = 1e-3`, `LR_ACTOR = 1e-4`, `LR_CRITIC = 1e-4`, `WEIGHT_DECAY = 0.0`, `N_LEARN_UPDATES = 10` and `N_TIME_STEPS = 20`\n",
    "\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "The Actor networks has three layers. The first two are fully connected with 96 units each with relu activation. The third layer has tanh activation for the action space. The network has state size as input dimention.\n",
    "\n",
    "The Critic networks has three layers. The first two are fully connected with 96 units each with relu activation. The first layer has state size as input dimention. The second layer accepts action size in addition to the output of the first layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement Ideas and Implementation\n",
    "\n",
    "The size of the Replay Buffer has material impacts on agents' performance. Large buffer breaks correlation between sequential samples but makes agents learn from long-ago history. Small buffer has the opposite effect. A technique calles [Combined Experience Replay (CER)](https://arxiv.org/abs/1712.01275) addresses this problem by using two buffers with different sizes simultaneously. \n",
    "\n",
    "In my implementation (shown below), Replay Buffer 1 has a large size (1000,000) and Replay Buffer 2 is tiny (20,000). The second buffer is design to keep most recent experiences. Older experiences are overwritten with new ones when the buffer size exceeds its maximum using Python FIFO data structure (deque). This implementation shows notable learning efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='architecture.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of rewards\n",
    "Without CER Replay Buffer, the performance goal is reached at episode 143.\n",
    "<img src=\"noCER_learning.png\">\n",
    "\n",
    "<b>With</b> CER Replay Buffer, the performance goal is reached at episode 106. Learning progress ramps up notably quicker.\n",
    "<img src=\"CER_learning.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
